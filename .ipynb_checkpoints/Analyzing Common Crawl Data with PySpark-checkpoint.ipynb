{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feeb65fd",
   "metadata": {},
   "source": [
    "# Common Crawl Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2744ebe9",
   "metadata": {},
   "source": [
    "https://www.codecademy.com/courses/big-data-pyspark/projects/pyspark-common-crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b8d3d3",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19d8d39",
   "metadata": {},
   "source": [
    "- data is from https://commoncrawl.org/\n",
    "- The Common Crawl is a non-profit organization that crawls, archives, and analyzes content on all public websites\n",
    "- data is publicly available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e25859",
   "metadata": {},
   "source": [
    "## Analyzing Common Crawl Data with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb0a22",
   "metadata": {},
   "source": [
    "### Initialize a new Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba9040cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca334eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['367855\\t172-in-addr\\tarpa\\t1', '367856\\taddr\\tarpa\\t1', '367857\\tamphic\\tarpa\\t1', '367858\\tbeta\\tarpa\\t1', '367859\\tcallic\\tarpa\\t1']\n"
     ]
    }
   ],
   "source": [
    "# read domains csv file into an rdd\n",
    "common_crawl_domain_counts = sc.textFile('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "# Display first few domains from the RDD\n",
    "print(common_crawl_domain_counts.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12516ce9",
   "metadata": {},
   "source": [
    "### Adjust output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66087f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmt_domain_graph_entry(entry):\n",
    "    \"\"\"\n",
    "    Formats a Common Crawl domain graph entry. Extracts the site_id, \n",
    "    top-level domain (tld), domain name, and subdomain count as seperate items.\n",
    "    \"\"\"\n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')        \n",
    "    return int(site_id), domain, tld, int(num_subdomains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e0b4247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(367855, '172-in-addr', 'arpa', 1), (367856, 'addr', 'arpa', 1), (367857, 'amphic', 'arpa', 1), (367858, 'beta', 'arpa', 1), (367859, 'callic', 'arpa', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Apply `fmt_domain_graph_entry` to the raw data RDD\n",
    "formatted_host_counts = common_crawl_domain_counts.map(fmt_domain_graph_entry)\n",
    "\n",
    "# Display the first few entries of the new RDD\n",
    "print(formatted_host_counts.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea3387ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def extract_subdomain_counts(entry):\n",
    "    \"\"\"\n",
    "    Extract the subdomain count from a Common Crawl domain graph entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the entry on delimiter ('\\t') into site_id, domain, tld, and num_subdomains\n",
    "    site_id, domain, tld, num_subdomains = entry.split('\\t')\n",
    "    \n",
    "    # return ONLY the num_subdomains\n",
    "    return int(num_subdomains)\n",
    "\n",
    "\n",
    "# Apply `extract_subdomain_counts` to the raw data RDD\n",
    "host_counts = common_crawl_domain_counts.map(extract_subdomain_counts)\n",
    "\n",
    "# Display the first few entries\n",
    "print(host_counts.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a09df5",
   "metadata": {},
   "source": [
    "### Calculate total nuber of subdomains across all domains in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ed3ddc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595466\n"
     ]
    }
   ],
   "source": [
    "# Reduce the RDD to a single value, the sum of subdomains, with a lambda function\n",
    "# as the reduce function\n",
    "total_host_counts = host_counts.reduce(lambda x,y: x+y)\n",
    "print(total_host_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c8ccf",
   "metadata": {},
   "source": [
    "### Stop the SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19742b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2ed13",
   "metadata": {},
   "source": [
    "## Exploring Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de30d0",
   "metadata": {},
   "source": [
    "### Initalize new SparkSession and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47b12f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e4d4733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----+---+\n",
      "|   _c0|        _c1| _c2|_c3|\n",
      "+------+-----------+----+---+\n",
      "|367855|172-in-addr|arpa|  1|\n",
      "|367856|       addr|arpa|  1|\n",
      "|367857|     amphic|arpa|  1|\n",
      "|367858|       beta|arpa|  1|\n",
      "|367859|     callic|arpa|  1|\n",
      "+------+-----------+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read csv file - it is stored as a DF as default\n",
    "common_crawl = spark.read \\\n",
    "               .option('format', 'csv') \\\n",
    "               .option('delimiter', '\\t') \\\n",
    "               .option('inferSchema', True) \\\n",
    "               .option('header', False) \\\n",
    "               .csv('./crawl/cc-main-limited-domains.csv')\n",
    "\n",
    "\n",
    "type(common_crawl)\n",
    "common_crawl.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2054c7",
   "metadata": {},
   "source": [
    "### Adjust data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f65dda",
   "metadata": {},
   "source": [
    "names of columns\n",
    "- site_id\n",
    "- domain\n",
    "- top_level_domain\n",
    "- num_subdomains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd717f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_crawl = common_crawl.toDF('site_id', 'domain', 'top_level_domain', 'num_subdomains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2134ee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------+\n",
      "|site_id|     domain|top_level_domain|num_subdomains|\n",
      "+-------+-----------+----------------+--------------+\n",
      "| 367855|172-in-addr|            arpa|             1|\n",
      "| 367856|       addr|            arpa|             1|\n",
      "| 367857|     amphic|            arpa|             1|\n",
      "| 367858|       beta|            arpa|             1|\n",
      "| 367859|     callic|            arpa|             1|\n",
      "+-------+-----------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('site_id', IntegerType(), True), StructField('domain', StringType(), True), StructField('top_level_domain', StringType(), True), StructField('num_subdomains', IntegerType(), True)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_crawl.show(5)\n",
    "common_crawl.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097ae68",
   "metadata": {},
   "source": [
    "## Reading and Writing Datasets to Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab5779",
   "metadata": {},
   "source": [
    "### Save DF as parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "07468ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_crawl.write.parquet('./results/common_crawl/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7882871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------+\n",
      "|site_id|     domain|top_level_domain|num_subdomains|\n",
      "+-------+-----------+----------------+--------------+\n",
      "| 367855|172-in-addr|            arpa|             1|\n",
      "| 367856|       addr|            arpa|             1|\n",
      "| 367857|     amphic|            arpa|             1|\n",
      "| 367858|       beta|            arpa|             1|\n",
      "| 367859|     callic|            arpa|             1|\n",
      "+-------+-----------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_crawl_domains = spark.read.parquet('./results/common_crawl/')\n",
    "common_crawl_domains.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4199fac",
   "metadata": {},
   "source": [
    "## Query in Domain Counts with PySpark DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c74df",
   "metadata": {},
   "source": [
    "### Create a temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97145249",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_crawl_domains.createOrReplaceTempView('common_crawl_domains_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693976e",
   "metadata": {},
   "source": [
    "### Calculate the total number of domains for each top-level domain in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "99f6bf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|top_level_domain|count|\n",
      "+----------------+-----+\n",
      "|          travel| 6313|\n",
      "|             map|   34|\n",
      "|             gov|15007|\n",
      "|             edu|18547|\n",
      "|            arpa|   11|\n",
      "|            jobs| 3893|\n",
      "|            post|  117|\n",
      "|            coop| 5319|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "common_crawl_domains_total_number = common_crawl_domains.groupBy('top_level_domain').count()\n",
    "common_crawl_domains_total_number.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6eaea323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|top_level_domain|count(1)|\n",
      "+----------------+--------+\n",
      "|travel          |6313    |\n",
      "|map             |34      |\n",
      "|gov             |15007   |\n",
      "|edu             |18547   |\n",
      "|arpa            |11      |\n",
      "|jobs            |3893    |\n",
      "|post            |117     |\n",
      "|coop            |5319    |\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using SQL\n",
    "common_crawl_domains_total_number_query = \"\"\"SELECT top_level_domain, count(*)\n",
    "                                             FROM common_crawl_domains_view\n",
    "                                             GROUP BY 1\n",
    "                                          \"\"\"\n",
    "\n",
    "spark.sql(common_crawl_domains_total_number_query).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608cbd39",
   "metadata": {},
   "source": [
    "### Calculate the total number of subdomains for each top-level domain in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "020fb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|top_level_domain|sum(num_subdomains)|\n",
      "+----------------+-------------------+\n",
      "|          travel|              10768|\n",
      "|             map|                 40|\n",
      "|             gov|              85354|\n",
      "|             edu|             484438|\n",
      "|            arpa|                 17|\n",
      "|            jobs|               6023|\n",
      "|            post|                143|\n",
      "|            coop|               8683|\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using DataFrame methods\n",
    "common_crawl_domains_total_number_subdomains = common_crawl_domains.groupBy('top_level_domain').sum('num_subdomains')\n",
    "common_crawl_domains_total_number_subdomains.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb6f360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|top_level_domain|sum(num_subdomains)|\n",
      "+----------------+-------------------+\n",
      "|travel          |10768              |\n",
      "|map             |40                 |\n",
      "|gov             |85354              |\n",
      "|edu             |484438             |\n",
      "|arpa            |17                 |\n",
      "|jobs            |6023               |\n",
      "|post            |143                |\n",
      "|coop            |8683               |\n",
      "+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the DataFrame using SQL\n",
    "common_crawl_domains_total_number_query = \"\"\"SELECT top_level_domain, sum(num_subdomains)\n",
    "                                             FROM common_crawl_domains_view\n",
    "                                             GROUP BY 1\n",
    "                                          \"\"\"\n",
    "\n",
    "spark.sql(common_crawl_domains_total_number_query).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
